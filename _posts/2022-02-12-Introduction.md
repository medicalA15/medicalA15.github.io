---
layout: inner
position: left
title: 'Introduction'
# date: 2016-02-20 21:15:00
# categories: development design
# tags: Jekyll Sass Bootstrap
# featured_image: '/img/posts/04_phantom-jekyll-1130x864-2x.png'
# project_link: 'https://github.com/jamigibbs'
# button_icon: 'flask'
# button_text: 'Visit Project'
lead_text: "The medical space is one that is fundamentally sensitive in terms of its effects on the lives of the patients within it. As a result, the transition into deep learning systems handling more sensitive information and tasks comes with the worries of those systems being compromised in some way and those vulnerabilities being responsible for harm to the lives and assets of people. Research on adversarial attacks shows cases where imperceptible adjustments to data within a deep learning system can cause said system to make incorrect predictions a majority of the time. 

Adversarial attacks are methods used to interfere with deep learning systems- with the intent of finding ways to misclassify data. These attacks generally come in the form of targeted and untargeted attacks. Targeted attacks entail manipulating data to output a desired outcome after feeding it to a model, while untargeted attacks focus on manipulating data to simply not be recognized as itâ€™s correct output.

In order to combat against such adversarial instances, there needs to be robust training done with these models in order to best protect against the methods that these attacks use on deep learning systems. In the scope of this paper, we will be looking into the methods of fast gradient signed method and projected gradient descent, two methods used in adversarial attacks to maximize loss functions and cause the affected system to make opposing predictions, in order to train our models against them and allow for stronger accuracy when faced with adversarial examples. 
"
---
